Initial observations DQN:
Running the testrunner multiple times resulted in two exactly equivalent runs.
By observation, the behavior of the agent was identical between the runs.
It followed the same path(by performing the same actions) and achieved the same scores
for the two episodes. Episode 1 in run 1 matched episode 1 in run 2 and the same for the second episode.
I'm suspecting the first versions of the network/algorithm may suffer from high variance.
The agent hasn't learned how to find bananas it's learned how to follow good paths that lead to bananas.
A few potential causes and fixes;
Perhaps the initial decay of epsilon is too high and it's minimum value is too low 
so that the agent performs too little initial and continued exploration.
Increase network size and depth to "hold more data" and allow more advanced strategies to be developed.
Decrease learning rate.
The agent may also get stuck between two bananas alternating between going left and right.


Installation:
cd python
pip install .


Performance:
CPU only: 
training round took 26.2 sec 
Episode 22 took 42.6 sec

GPU CPU hybrid:
training round took 0.5 sec
Episode 163 took 38.3 sec 


Results:
Distributional DQN v1:
eps_decay = 0.995
eps_min = 0.01
Episode 994 Score: 4.0 frame 298500 took 53.4 sec epsilon 0.006822842436385474
Episode 995 Score: 0.0 frame 298800 took 53.6 sec epsilon 0.006788728224203546
Episode 996 Score: 1.0 frame 299100 took 53.4 sec epsilon 0.006754784583082528
Episode 997 Score: 3.0 frame 299400 took 53.4 sec epsilon 0.006721010660167116
Episode 998 Score: 0.0 frame 299700 took 53.5 sec epsilon 0.00668740560686628
Episode 999 Score: 2.0 frame 300000 took 53.2 sec epsilon 0.006653968578831948

Distributional Double DQN /w multi step bootstrap targets v1:
eps_decay = 0.995
eps_min = 0.01
Episode 994 Score: 0.0 frame 298500 took 51.3 sec epsilon 0.01
Episode 995 Score: 4.0 frame 298800 took 51.7 sec epsilon 0.01
Episode 996 Score: 1.0 frame 299100 took 51.3 sec epsilon 0.01
Episode 997 Score: 2.0 frame 299400 took 51.3 sec epsilon 0.01
Episode 998 Score: 0.0 frame 299700 took 51.1 sec epsilon 0.01
Episode 999 Score: 2.0 frame 300000 took 51.3 sec epsilon 0.01

Distributional Double DQN /w multi step bootstrap targets and prioritized experience replay v1:
eps_decay = 0.995
eps_min = 0.01
buffer_size = 10000
Episode 994 Score: 2.0 frame 298500 took 43.8 sec sampling_beta 0.997 epsilon 0.0100
Episode 995 Score: 1.0 frame 298800 took 43.8 sec sampling_beta 0.998 epsilon 0.0100
Episode 996 Score: 2.0 frame 299100 took 43.7 sec sampling_beta 0.998 epsilon 0.0100
Episode 997 Score: 5.0 frame 299400 took 43.8 sec sampling_beta 0.999 epsilon 0.0100
Episode 998 Score: 0.0 frame 299700 took 43.9 sec sampling_beta 0.999 epsilon 0.0100
Episode 999 Score: 2.0 frame 300000 took 43.8 sec sampling_beta 1.000 epsilon 0.0100